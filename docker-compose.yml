version: "3.8"

services:
  # --- ZOOKEEPER & KAFKA ---
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    restart: always
    networks:
      - bigdata_network

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092" # Akses dari luar Docker
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "recipe-stream:1:1" # Topik untuk data resep
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Untuk single broker setup
      KAFKA_MESSAGE_MAX_BYTES: 2000000 # Naikkan jika pesan resep besar (default 1MB)
      KAFKA_REPLICA_FETCH_MAX_BYTES: 2000000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper
    restart: always
    networks:
      - bigdata_network

  # --- HADOOP CLUSTER (Opsional jika tidak ingin HDFS, tapi baik untuk simulasi) ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" # UI Namenode
      - "9000:9000" # Port FS
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data_staging:/data_staging # Shared volume
    environment:
      - CLUSTER_NAME=test
    networks:
      - bigdata_network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    networks:
      - bigdata_network

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    ports:
      - "8088:8088" # UI YARN ResourceManager
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    depends_on:
      - namenode
      - datanode
    networks:
      - bigdata_network

  # --- SPARK CLUSTER ---
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3 # Gunakan versi Spark yang lebih spesifik
    container_name: spark-master
    ports:
      - "8081:8080" # UI Spark Master (seringnya 8080 di dalam, map ke 8081 di host)
      - "7077:7077" # Port Spark Master
    volumes:
      - ./spark_recipe_processor.py:/app/spark_recipe_processor.py
      - ./data_staging:/data_staging # Akses ke batch mentah dan output model
    depends_on:
      - kafka
      - namenode # Jika Spark berinteraksi dengan HDFS
      - resourcemanager # Jika Spark berjalan di YARN (tidak untuk setup Standalone ini)
    restart: always
    networks:
      - bigdata_network

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3 # Sesuaikan dengan versi master
    container_name: spark-worker-1
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_MEMORY=1g"
      - "SPARK_WORKER_CORES=1"
    volumes:
      - ./spark_recipe_processor.py:/app/spark_recipe_processor.py # Opsional, master yg submit
      - ./data_staging:/data_staging
    depends_on:
      - spark-master
    restart: always
    networks:
      - bigdata_network

  # --- KAFKA PRODUCER UNTUK RESEP ---
  producer-recipe:
    image: python:3.9-slim
    container_name: producer-recipe
    working_dir: /app
    volumes:
      - ./producer_recipe.py:/app/producer_recipe.py
      - ./requirements_producer_recipe.txt:/app/requirements.txt
      - ./recipes_data.csv:/app/recipes_data.csv # Dataset CSV Anda
    command: >
      sh -c "pip install --no-cache-dir -r requirements.txt &&
             python /app/producer_recipe.py"
    depends_on:
      - kafka
    restart: on-failure
    networks:
      - bigdata_network

  # --- KAFKA CONSUMER (Python Non-Spark, untuk batching data mentah) ---
  kafka-simple-consumer:
    image: python:3.9-slim
    container_name: kafka-simple-consumer
    working_dir: /app
    volumes:
      - ./kafka_simple_consumer.py:/app/kafka_simple_consumer.py
      - ./requirements_consumer_simple.txt:/app/requirements.txt
      - ./data_staging:/data_staging # Menyimpan batch data mentah
    command: >
      sh -c "pip install --no-cache-dir -r requirements.txt &&
             python /app/kafka_simple_consumer.py"
    depends_on:
      - kafka
    restart: always
    networks:
      - bigdata_network
      
  # --- API SERVICE UNTUK REKOMENDASI ---
  recipe-api:
    image: python:3.9-slim
    container_name: recipe-api
    working_dir: /app
    ports:
      - "5000:5000" # Port API
    volumes:
      - ./api_server.py:/app/api_server.py
      - ./requirements_api.txt:/app/requirements.txt
      - ./data_staging:/data_staging # Mengakses "model" (data Parquet)
    command: >
      sh -c "pip install --no-cache-dir -r requirements.txt &&
             python /app/api_server.py"
    depends_on: 
      - kafka # Secara infrastruktur, API bisa start setelah Kafka
    restart: always
    networks:
      - bigdata_network

volumes:
  hadoop_namenode:
  hadoop_datanode:

networks:
  bigdata_network:
    driver: bridge